#include <tbb/spin_mutex.h>
#include <tbb/parallel_reduce.h>


#include "../Loop.cpph"

/*


template <typename F>
void tarch::multicore::parallelReduce(
  const tarch::multicore::dForRange<1>&  range,
  F&                                     function
) {
  std::vector< dForRange<1> > ranges = range.getMinimalRanges();
  tbb::spin_mutex mutex;

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
	  F copyOfFunction(function);
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++) {
       	  tarch::la::Vector<1,int> loc;
          loc(0) = i0;
          copyOfFunction(ranges[i](loc));
        }
      }

      tbb::spin_mutex::scoped_lock lock(mutex);
      copyOfFunction.mergeIntoMasterThread();
    }
  );
}


template <typename F>
void tarch::multicore::parallelReduce(
  const tarch::multicore::dForRange<2>&  range,
  F&                                     function
) {
  std::vector< dForRange<2> > ranges = range.getMinimalRanges();
  tbb::spin_mutex mutex;

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
	  F copyOfFunction(function);
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++) {
       	  tarch::la::Vector<2,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          copyOfFunction(ranges[i](loc));
        }
      }

      tbb::spin_mutex::scoped_lock lock(mutex);
      copyOfFunction.mergeIntoMasterThread();
    }
  );
}


template <typename F>
void tarch::multicore::parallelReduce(
  const tarch::multicore::dForRange<3>&  range,
  F&                                     function
) {
  std::vector< dForRange<3> > ranges = range.getMinimalRanges();
  tbb::spin_mutex mutex;

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
	  F copyOfFunction(function);
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++)
        for (int i2=0; i2<ranges[i].getRange()(2); i2++) {
       	  tarch::la::Vector<3,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          loc(2) = i2;
          copyOfFunction(ranges[i](loc));
        }
      }

      tbb::spin_mutex::scoped_lock lock(mutex);
      copyOfFunction.mergeIntoMasterThread();
    }
  );
}


template <typename F>
void tarch::multicore::parallelReduce(
  const tarch::multicore::dForRange<4>&  range,
  F&                                     function
) {
  std::vector< dForRange<4> > ranges = range.getMinimalRanges();
  tbb::spin_mutex mutex;

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
	  F copyOfFunction(function);
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++)
        for (int i2=0; i2<ranges[i].getRange()(2); i2++)
        for (int i3=0; i3<ranges[i].getRange()(3); i3++) {
       	  tarch::la::Vector<4,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          loc(2) = i2;
          loc(3) = i3;
          copyOfFunction(ranges[i](loc));
        }
      }

      tbb::spin_mutex::scoped_lock lock(mutex);
      copyOfFunction.mergeIntoMasterThread();
    }
  );
}



*
 * Per default, TBB versions greater than TBB 2.2 do use the auto partitioner.
 * An auto partitioner is similar to OpenMP's guided: It splits up the problem
 * such that every thread is busy. Whenever work is stolen, the algorithm tries
 * to subdivide is further.
 *
 * We found that this is inefficient for Peano's applications - notably as we
 * deploy the responsibility to find good grain sizes (and thus minimalistic
 * overheads with good balancing) to the oracles.
 *
 * A static_partitioner() divides, if possible, all ranges such that all
 * threads are busy. As a consequence, work stealing is indirectly disabled. We
 * found this variant to scale better than the auto strategy once we have found
 * proper grain sizes.
 *
 * The optimal strategy however seems to be the simple_partitioner which splits
 * up each range until ranges cannot be divided further. As a result, work is
 * stolen, but no further range subdivision is happening.

template <typename F>
void tarch::multicore::parallelFor(
  const tarch::multicore::dForRange<1>&  range,
  F&                                     function
) {
  std::vector< dForRange<1> > ranges = range.getMinimalRanges();

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++) {
      	  tarch::la::Vector<1,int> loc;
          loc(0) = i0;
          function(ranges[i](loc));
        }
      }
    },
	tbb::simple_partitioner()
  );
}


template <typename F>
void tarch::multicore::parallelFor(
  const tarch::multicore::dForRange<2>&  range,
  F&                                     function
) {
  std::vector< dForRange<2> > ranges = range.getMinimalRanges();

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++) {
       	  tarch::la::Vector<2,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          function(ranges[i](loc));
        }
      }
    },
	tbb::simple_partitioner()
  );
}


template <typename F>
void tarch::multicore::parallelFor(
  const tarch::multicore::dForRange<3>&  range,
  F&                                     function
) {
  std::vector< dForRange<3> > ranges = range.getMinimalRanges();

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++)
        for (int i2=0; i2<ranges[i].getRange()(2); i2++) {
       	  tarch::la::Vector<3,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          loc(2) = i2;
          function(ranges[i](loc));
        }
      }
    },
	tbb::simple_partitioner()
  );
}


template <typename F>
void tarch::multicore::parallelFor(
  const tarch::multicore::dForRange<4>&  range,
  F&                                     function
) {
  std::vector< dForRange<4> > ranges = range.getMinimalRanges();

  tbb::parallel_for(
    tbb::blocked_range<size_t>(0,ranges.size()),
    [&](const tbb::blocked_range<size_t>& r) {
      for(size_t i=r.begin(); i!=r.end(); ++i) {
        for (int i0=0; i0<ranges[i].getRange()(0); i0++)
        for (int i1=0; i1<ranges[i].getRange()(1); i1++)
        for (int i2=0; i2<ranges[i].getRange()(2); i2++)
        for (int i3=0; i3<ranges[i].getRange()(3); i3++) {
       	  tarch::la::Vector<4,int> loc;
          loc(0) = i0;
          loc(1) = i1;
          loc(2) = i2;
          loc(3) = i3;
          function(ranges[i](loc));
        }
      }
    },
	tbb::simple_partitioner()
  );
}

*/


